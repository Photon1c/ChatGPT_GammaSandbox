{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6997e388",
   "metadata": {},
   "source": [
    "# Pending Task: Fix VQQNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = input(\"Enter stocks to analyze: \")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2020-01-26\", \"2023-04-10\", adjustment='raw').df\n",
    "\n",
    "\n",
    "\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['volume']\n",
    "\n",
    "\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"No rows with complete data found in the dataframe\")\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-11').tz_localize(pytz.utc)\n",
    "    x_train = np.array([], dtype=np.float32)\n",
    "    y_train = np.array([], dtype=np.float32)\n",
    "    x_test = np.array([], dtype=np.float32)\n",
    "    y_test = np.array([], dtype=np.float32)\n",
    "    if forward_test_date > df_x.index.max().tz_convert(pytz.utc):\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "        \n",
    " \n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                            df_y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state=42)\n",
    "    if forward_test_date <= df_x.index.max().tz_convert(pytz.utc):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                        df_y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42)\n",
    "        x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "        y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "        x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "        y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "    else:\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "\n",
    "\n",
    "    x_train = np.expand_dims(x_train, 1).astype(np.float32)\n",
    "    y_train = np.expand_dims(y_train, 1).astype(np.float32)\n",
    "    x_validation = np.expand_dims(x_test, 1).astype(np.float32)\n",
    "    y_validation = np.expand_dims(y_test, 1).astype(np.float32)\n",
    "\n",
    "    qnn_model = VQNNModel()\n",
    "    qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1=0.9,\n",
    "                                                         beta_2=0.999,\n",
    "                                                         epsilon=1e-07),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[\"mean_squared_error\"],\n",
    "                      run_eagerly=True)\n",
    "\n",
    "    log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    qnn_model.fit(\n",
    "    x_train, y_train, epochs=1, batch_size=1,\n",
    "                  callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "    num_of_features = fdf_x_scaled.shape[1]\n",
    "    qnn_predictions = []\n",
    "    for entry in fdf_x_scaled.iterrows():\n",
    "        fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "        qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "        qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "    signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    sub_qr = QuantumRegister(10)\n",
    "    sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "    sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "    sub_circuit.barrier()\n",
    "\n",
    "    # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "    sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "    ])\n",
    "\n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bb1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is meant to debug cell above\n",
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from tensor_layer_expermental import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = \"INTC\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2023-03-31\", \"2023-03-31\", adjustment='raw').df\n",
    "    \n",
    "    \n",
    "    df['HL_DELTA'] = df['high'] - df['low']\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(df)\n",
    "    \n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "\n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['VOLUME']\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-10').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        # Use the trained model to make predictions\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "    df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2025-03-01').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "                # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "primaryfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master Code\n",
    "#Source and credit: https://github.com/MSKX/tensorflow-quantum_dense\n",
    "#Confirm successful import of packages\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "#Confirm successful import of input data, instead of refinitive.dataplatform.eikon API\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime as date\n",
    "\n",
    "\n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "# read input data from CSV file\n",
    "df = pd.read_csv('datasets/test.csv')\n",
    "\n",
    "# convert date column to datetime and set as index\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df.set_index('DATE', inplace=True)\n",
    "df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Create 3 dimensional Tensor\n",
    "df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "df_y = df['TP1_RETURNS']\n",
    "df_x.dropna(inplace=True)\n",
    "\n",
    "df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "forward_test_date = '2022-10-01'\n",
    "\n",
    "fdf_x = df_x.loc[forward_test_date:]\n",
    "fdf_y = df_y.loc[forward_test_date:]\n",
    "df_x = df_x.loc[:forward_test_date]\n",
    "df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "fdf_prx = df.loc[forward_test_date:]['CLOSE']\n",
    "fdf_y_len = len(fdf_y)\n",
    "\n",
    "df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                    df_y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "qnn_model = VQNNModel()\n",
    "qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.999,\n",
    "                                                     epsilon=1e-07),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[\"mean_squared_error\"])\n",
    "\n",
    "qnn_model.run_eagerly = True\n",
    "\n",
    "##patch\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "qnn_model.fit(\n",
    "x_train, y_train, epochs=1, batch_size=1,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
