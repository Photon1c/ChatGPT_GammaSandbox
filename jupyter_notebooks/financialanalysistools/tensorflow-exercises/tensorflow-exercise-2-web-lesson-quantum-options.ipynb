{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6997e388",
   "metadata": {},
   "source": [
    "# Pending Task: Fix VQQNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = input(\"Enter stocks to analyze: \")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2020-01-26\", \"2023-04-10\", adjustment='raw').df\n",
    "\n",
    "\n",
    "\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['volume']\n",
    "\n",
    "\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"No rows with complete data found in the dataframe\")\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-11').tz_localize(pytz.utc)\n",
    "    x_train = np.array([], dtype=np.float32)\n",
    "    y_train = np.array([], dtype=np.float32)\n",
    "    x_test = np.array([], dtype=np.float32)\n",
    "    y_test = np.array([], dtype=np.float32)\n",
    "    if forward_test_date > df_x.index.max().tz_convert(pytz.utc):\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "        \n",
    " \n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                            df_y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state=42)\n",
    "    if forward_test_date <= df_x.index.max().tz_convert(pytz.utc):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                        df_y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42)\n",
    "        x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "        y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "        x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "        y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "    else:\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "\n",
    "\n",
    "    x_train = np.expand_dims(x_train, 1).astype(np.float32)\n",
    "    y_train = np.expand_dims(y_train, 1).astype(np.float32)\n",
    "    x_validation = np.expand_dims(x_test, 1).astype(np.float32)\n",
    "    y_validation = np.expand_dims(y_test, 1).astype(np.float32)\n",
    "\n",
    "    qnn_model = VQNNModel()\n",
    "    qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1=0.9,\n",
    "                                                         beta_2=0.999,\n",
    "                                                         epsilon=1e-07),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[\"mean_squared_error\"],\n",
    "                      run_eagerly=True)\n",
    "\n",
    "    log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    qnn_model.fit(\n",
    "    x_train, y_train, epochs=1, batch_size=1,\n",
    "                  callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "    num_of_features = fdf_x_scaled.shape[1]\n",
    "    qnn_predictions = []\n",
    "    for entry in fdf_x_scaled.iterrows():\n",
    "        fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "        qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "        qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "    signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    sub_qr = QuantumRegister(10)\n",
    "    sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "    sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "    sub_circuit.barrier()\n",
    "\n",
    "    # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "    sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "    ])\n",
    "\n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483bb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              open     high     low   close    volume  \\\n",
      "timestamp                                                               \n",
      "2023-03-31 12:00:00+00:00  32.0700  32.2200  31.910  31.980    193101   \n",
      "2023-03-31 13:00:00+00:00  31.9800  32.3300  31.730  32.110   5734329   \n",
      "2023-03-31 14:00:00+00:00  32.1100  32.6200  32.110  32.585  10117957   \n",
      "2023-03-31 15:00:00+00:00  32.5800  32.6800  32.520  32.590   5716578   \n",
      "2023-03-31 16:00:00+00:00  32.5850  32.9000  32.500  32.820   5537337   \n",
      "2023-03-31 17:00:00+00:00  32.8150  32.8700  32.705  32.775   4641895   \n",
      "2023-03-31 18:00:00+00:00  32.7739  32.8650  32.615  32.675   5024295   \n",
      "2023-03-31 19:00:00+00:00  32.6750  32.8000  32.530  32.645  11803141   \n",
      "2023-03-31 20:00:00+00:00  32.6700  32.6996  32.560  32.640   1655629   \n",
      "2023-03-31 21:00:00+00:00  32.6400  32.6500  32.620  32.640      6960   \n",
      "2023-03-31 22:00:00+00:00  32.6200  32.6800  32.610  32.680      9663   \n",
      "\n",
      "                           trade_count       vwap  HL_DELTA   RETURNS  \\\n",
      "timestamp                                                               \n",
      "2023-03-31 12:00:00+00:00         2305  32.065453    0.3100 -0.004049   \n",
      "2023-03-31 13:00:00+00:00        29929  32.032425    0.6000  0.004065   \n",
      "2023-03-31 14:00:00+00:00        52017  32.429379    0.5100  0.014793   \n",
      "2023-03-31 15:00:00+00:00        30682  32.614343    0.1600  0.000153   \n",
      "2023-03-31 16:00:00+00:00        28974  32.741719    0.4000  0.007057   \n",
      "2023-03-31 17:00:00+00:00        24168  32.784606    0.1650 -0.001371   \n",
      "2023-03-31 18:00:00+00:00        24431  32.738547    0.2500 -0.003051   \n",
      "2023-03-31 19:00:00+00:00        58194  32.676481    0.2700 -0.000918   \n",
      "2023-03-31 20:00:00+00:00          426  32.669394    0.1396 -0.000153   \n",
      "2023-03-31 21:00:00+00:00           63  32.645853    0.0300  0.000000   \n",
      "2023-03-31 22:00:00+00:00           85  32.660745    0.0700  0.001225   \n",
      "\n",
      "                           PRX_MA_ND  VOLATILITY  TP1_RETURNS  \n",
      "timestamp                                                      \n",
      "2023-03-31 12:00:00+00:00     32.086    0.064265     0.004065  \n",
      "2023-03-31 13:00:00+00:00     32.078    0.056303     0.014793  \n",
      "2023-03-31 14:00:00+00:00     32.173    0.236421     0.000153  \n",
      "2023-03-31 15:00:00+00:00     32.275    0.290172     0.007057  \n",
      "2023-03-31 16:00:00+00:00     32.417    0.355591    -0.001371  \n",
      "2023-03-31 17:00:00+00:00     32.576    0.281323    -0.003051  \n",
      "2023-03-31 18:00:00+00:00     32.689    0.106501    -0.000918  \n",
      "2023-03-31 19:00:00+00:00     32.701    0.094565    -0.000153  \n",
      "2023-03-31 20:00:00+00:00     32.711    0.081655     0.000000  \n",
      "2023-03-31 21:00:00+00:00     32.675    0.057771     0.001225  \n",
      "2023-03-31 22:00:00+00:00     32.656    0.019812    -0.000612  \n"
     ]
    }
   ],
   "source": [
    "#This cell is meant to debug cell above\n",
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from tensor_layer_expermental import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = \"INTC\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2023-03-31\", \"2023-03-31\", adjustment='raw').df\n",
    "    \n",
    "    \n",
    "    df['HL_DELTA'] = df['high'] - df['low']\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(df)\n",
    "    \n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "\n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['VOLUME']\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-10').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        # Use the trained model to make predictions\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "    df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2025-03-01').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "                # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "primaryfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4170c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_25812\\2454655176.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_x.dropna(inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m\n\u001b[0;32m     89\u001b[0m qnn_model\u001b[38;5;241m.\u001b[39mrun_eagerly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m##patch\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# assuming `df_x_test` is your test data\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# create a scaler object and fit it on `x_train` data\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mMinMaxScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\u001b[39;00m\n\u001b[0;32m     97\u001b[0m x_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(df_x_test)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:427\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:466\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    465\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 466\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m     )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    921\u001b[0m     _assert_all_finite(\n\u001b[0;32m    922\u001b[0m         array,\n\u001b[0;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "#Master Code\n",
    "#Source and credit: https://github.com/MSKX/tensorflow-quantum_dense\n",
    "#Confirm successful import of packages\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "#Confirm successful import of input data, instead of refinitive.dataplatform.eikon API\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime as date\n",
    "\n",
    "\n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "# read input data from CSV file\n",
    "df = pd.read_csv('datasets/test.csv')\n",
    "\n",
    "# convert date column to datetime and set as index\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df.set_index('DATE', inplace=True)\n",
    "df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Create 3 dimensional Tensor\n",
    "df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "df_y = df['TP1_RETURNS']\n",
    "df_x.dropna(inplace=True)\n",
    "\n",
    "df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "forward_test_date = '2022-10-01'\n",
    "\n",
    "fdf_x = df_x.loc[forward_test_date:]\n",
    "fdf_y = df_y.loc[forward_test_date:]\n",
    "df_x = df_x.loc[:forward_test_date]\n",
    "df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "fdf_prx = df.loc[forward_test_date:]['CLOSE']\n",
    "fdf_y_len = len(fdf_y)\n",
    "\n",
    "df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                    df_y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "qnn_model = VQNNModel()\n",
    "qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.999,\n",
    "                                                     epsilon=1e-07),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[\"mean_squared_error\"])\n",
    "\n",
    "qnn_model.run_eagerly = True\n",
    "\n",
    "##patch\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "qnn_model.fit(\n",
    "x_train, y_train, epochs=1, batch_size=1,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
