{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6997e388",
   "metadata": {},
   "source": [
    "# Pending Task: Fix VQQNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = input(\"Enter stocks to analyze: \")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2020-01-26\", \"2023-04-10\", adjustment='raw').df\n",
    "\n",
    "\n",
    "\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['volume']\n",
    "\n",
    "\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"No rows with complete data found in the dataframe\")\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-11').tz_localize(pytz.utc)\n",
    "    x_train = np.array([], dtype=np.float32)\n",
    "    y_train = np.array([], dtype=np.float32)\n",
    "    x_test = np.array([], dtype=np.float32)\n",
    "    y_test = np.array([], dtype=np.float32)\n",
    "    if forward_test_date > df_x.index.max().tz_convert(pytz.utc):\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "        \n",
    " \n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                            df_y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state=42)\n",
    "    if forward_test_date <= df_x.index.max().tz_convert(pytz.utc):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                        df_y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42)\n",
    "        x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "        y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "        x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "        y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "    else:\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "\n",
    "\n",
    "    x_train = np.expand_dims(x_train, 1).astype(np.float32)\n",
    "    y_train = np.expand_dims(y_train, 1).astype(np.float32)\n",
    "    x_validation = np.expand_dims(x_test, 1).astype(np.float32)\n",
    "    y_validation = np.expand_dims(y_test, 1).astype(np.float32)\n",
    "\n",
    "    qnn_model = VQNNModel()\n",
    "    qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1=0.9,\n",
    "                                                         beta_2=0.999,\n",
    "                                                         epsilon=1e-07),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[\"mean_squared_error\"],\n",
    "                      run_eagerly=True)\n",
    "\n",
    "    log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    qnn_model.fit(\n",
    "    x_train, y_train, epochs=1, batch_size=1,\n",
    "                  callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "    num_of_features = fdf_x_scaled.shape[1]\n",
    "    qnn_predictions = []\n",
    "    for entry in fdf_x_scaled.iterrows():\n",
    "        fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "        qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "        qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "    signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    sub_qr = QuantumRegister(10)\n",
    "    sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "    sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "    sub_circuit.barrier()\n",
    "\n",
    "    # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "    sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "    ])\n",
    "\n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483bb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              open     high     low   close    volume  \\\n",
      "timestamp                                                               \n",
      "2023-03-31 12:00:00+00:00  32.0700  32.2200  31.910  31.980    193101   \n",
      "2023-03-31 13:00:00+00:00  31.9800  32.3300  31.730  32.110   5734329   \n",
      "2023-03-31 14:00:00+00:00  32.1100  32.6200  32.110  32.585  10117957   \n",
      "2023-03-31 15:00:00+00:00  32.5800  32.6800  32.520  32.590   5716578   \n",
      "2023-03-31 16:00:00+00:00  32.5850  32.9000  32.500  32.820   5537337   \n",
      "2023-03-31 17:00:00+00:00  32.8150  32.8700  32.705  32.775   4641895   \n",
      "2023-03-31 18:00:00+00:00  32.7739  32.8650  32.615  32.675   5024295   \n",
      "2023-03-31 19:00:00+00:00  32.6750  32.8000  32.530  32.645  11803141   \n",
      "2023-03-31 20:00:00+00:00  32.6700  32.6996  32.560  32.640   1655629   \n",
      "2023-03-31 21:00:00+00:00  32.6400  32.6500  32.620  32.640      6960   \n",
      "2023-03-31 22:00:00+00:00  32.6200  32.6800  32.610  32.680      9663   \n",
      "\n",
      "                           trade_count       vwap  HL_DELTA   RETURNS  \\\n",
      "timestamp                                                               \n",
      "2023-03-31 12:00:00+00:00         2305  32.065453    0.3100 -0.004049   \n",
      "2023-03-31 13:00:00+00:00        29929  32.032425    0.6000  0.004065   \n",
      "2023-03-31 14:00:00+00:00        52017  32.429379    0.5100  0.014793   \n",
      "2023-03-31 15:00:00+00:00        30682  32.614343    0.1600  0.000153   \n",
      "2023-03-31 16:00:00+00:00        28974  32.741719    0.4000  0.007057   \n",
      "2023-03-31 17:00:00+00:00        24168  32.784606    0.1650 -0.001371   \n",
      "2023-03-31 18:00:00+00:00        24431  32.738547    0.2500 -0.003051   \n",
      "2023-03-31 19:00:00+00:00        58194  32.676481    0.2700 -0.000918   \n",
      "2023-03-31 20:00:00+00:00          426  32.669394    0.1396 -0.000153   \n",
      "2023-03-31 21:00:00+00:00           63  32.645853    0.0300  0.000000   \n",
      "2023-03-31 22:00:00+00:00           85  32.660745    0.0700  0.001225   \n",
      "\n",
      "                           PRX_MA_ND  VOLATILITY  TP1_RETURNS  \n",
      "timestamp                                                      \n",
      "2023-03-31 12:00:00+00:00     32.086    0.064265     0.004065  \n",
      "2023-03-31 13:00:00+00:00     32.078    0.056303     0.014793  \n",
      "2023-03-31 14:00:00+00:00     32.173    0.236421     0.000153  \n",
      "2023-03-31 15:00:00+00:00     32.275    0.290172     0.007057  \n",
      "2023-03-31 16:00:00+00:00     32.417    0.355591    -0.001371  \n",
      "2023-03-31 17:00:00+00:00     32.576    0.281323    -0.003051  \n",
      "2023-03-31 18:00:00+00:00     32.689    0.106501    -0.000918  \n",
      "2023-03-31 19:00:00+00:00     32.701    0.094565    -0.000153  \n",
      "2023-03-31 20:00:00+00:00     32.711    0.081655     0.000000  \n",
      "2023-03-31 21:00:00+00:00     32.675    0.057771     0.001225  \n",
      "2023-03-31 22:00:00+00:00     32.656    0.019812    -0.000612  \n"
     ]
    }
   ],
   "source": [
    "#This cell is meant to debug cell above\n",
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from tensor_layer_expermental import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = \"INTC\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2023-03-31\", \"2023-03-31\", adjustment='raw').df\n",
    "    \n",
    "    \n",
    "    df['HL_DELTA'] = df['high'] - df['low']\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(df)\n",
    "    \n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "\n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['VOLUME']\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-10').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        # Use the trained model to make predictions\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "    df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2025-03-01').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "                # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "primaryfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4170c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_25812\\3903010876.py:18: DeprecationWarning: The package qiskit.providers.ibmq is being deprecated. Please see https://ibm.biz/provider_migration_guide to get instructions on how to migrate to qiskit-ibm-provider (https://github.com/Qiskit/qiskit-ibm-provider) and qiskit-ibm-runtime (https://github.com/Qiskit/qiskit-ibm-runtime).\n",
      "  from QuantumDense import VQNNModel\n",
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_25812\\3903010876.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_x.dropna(inplace=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'quantum_layer' (type QuantumLayer).\n\n'TypeError' object has no attribute 'message'\n\nCall arguments received by layer 'quantum_layer' (type QuantumLayer):\n  • inputs=tf.Tensor(shape=(1, 1, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(log_dir)\n\u001b[0;32m     94\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mqnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\mainline\\machinelearning\\ChatGPT_ThetaSandbox\\jupyter_notebooks\\financialanalysistools\\tensorflow-exercises\\QuantumDense.py:251\u001b[0m, in \u001b[0;36mVQNNModel.call\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_layer(input_tensor, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantum_layer(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\mainline\\machinelearning\\ChatGPT_ThetaSandbox\\jupyter_notebooks\\financialanalysistools\\tensorflow-exercises\\QuantumDense.py:190\u001b[0m, in \u001b[0;36mQuantumLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_parameter_shift_gradient_flow:\n\u001b[0;32m    189\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_p)\n\u001b[1;32m--> 190\u001b[0m     qubit_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mquantum_execute(tf\u001b[38;5;241m.\u001b[39mreshape(output, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_phi)\n\u001b[0;32m    191\u001b[0m     qubit_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(qubit_output), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits))\n\u001b[0;32m    192\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (qubit_output \u001b[38;5;241m-\u001b[39m output)\n",
      "File \u001b[1;32mD:\\mainline\\machinelearning\\ChatGPT_ThetaSandbox\\jupyter_notebooks\\financialanalysistools\\tensorflow-exercises\\QuantumDense.py:119\u001b[0m, in \u001b[0;36mQiskitCircuitModule.quantum_execute\u001b[1;34m(self, probabilities, phases)\u001b[0m\n\u001b[0;32m    113\u001b[0m     qubit_set_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_qubit_set_probabilities(result)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QiskitCircuitModuleException(\n\u001b[0;32m    116\u001b[0m         QiskitCircuitModuleExceptionData(\u001b[38;5;28mstr\u001b[39m({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    117\u001b[0m                                  strftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124m                                 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantum_execute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;124m                                 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m})))\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qubit_set_probabilities\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer 'quantum_layer' (type QuantumLayer).\n\n'TypeError' object has no attribute 'message'\n\nCall arguments received by layer 'quantum_layer' (type QuantumLayer):\n  • inputs=tf.Tensor(shape=(1, 1, 3), dtype=float32)"
     ]
    }
   ],
   "source": [
    "#Master Code\n",
    "#Source and credit: https://github.com/MSKX/tensorflow-quantum_dense\n",
    "#Confirm successful import of packages\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "import matplotlib.dates as plt_dates\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from QuantumDense import VQNNModel\n",
    "\n",
    "#Confirm successful import of input data, instead of refinitive.dataplatform.eikon API\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime as date\n",
    "\n",
    "\n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "# read input data from CSV file\n",
    "df = pd.read_csv('datasets/test.csv')\n",
    "\n",
    "# convert date column to datetime and set as index\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df.set_index('DATE', inplace=True)\n",
    "df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Create 3 dimensional Tensor\n",
    "df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "df_y = df['TP1_RETURNS']\n",
    "df_x.dropna(inplace=True)\n",
    "\n",
    "df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "forward_test_date = '2022-10-01'\n",
    "\n",
    "fdf_x = df_x.loc[forward_test_date:]\n",
    "fdf_y = df_y.loc[forward_test_date:]\n",
    "df_x = df_x.loc[:forward_test_date]\n",
    "df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "fdf_prx = df.loc[forward_test_date:]['CLOSE']\n",
    "fdf_y_len = len(fdf_y)\n",
    "\n",
    "df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                    df_y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "qnn_model = VQNNModel()\n",
    "qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.999,\n",
    "                                                     epsilon=1e-07),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[\"mean_squared_error\"])\n",
    "\n",
    "qnn_model.run_eagerly = True\n",
    "\n",
    "##patch\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "qnn_model.fit(\n",
    "x_train, y_train, epochs=1, batch_size=1,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# assuming `df_x_test` is your test data\n",
    "# create a scaler object and fit it on `x_train` data\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "\n",
    "# use `transform()` to scale `df_x_test` using the same scaling parameters as `x_train`\n",
    "x_test_scaled = scaler.transform(df_x_test)\n",
    "\n",
    "# assuming `qnn_model` is your trained model\n",
    "# use `predict()` to make predictions on the scaled test data\n",
    "y_pred = qnn_model.predict(x_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
