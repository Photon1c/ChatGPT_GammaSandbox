{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6997e388",
   "metadata": {},
   "source": [
    "# Pending Task: Fix VQQNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = input(\"Enter stocks to analyze: \")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VQNNModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_of_features, **kwargs):\n",
    "        super(VQNNModel, self).__init__(**kwargs)\n",
    "\n",
    "        # Define qubit layers here\n",
    "        self.dense1 = tf.keras.layers.Dense(num_of_features)\n",
    "        self.qft = QFT(num_of_features)\n",
    "        self.qnn = QNN(num_of_features, 2, 1)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.qft(x)\n",
    "        rho = tf.reshape(x, [-1, 2 ** num_of_features, 1])\n",
    "        return self.qnn([rho, self.qnn.get_ideal_rho(x.shape[0])])\n",
    "\n",
    "num_of_features = 4  # change this to the number of features in your dataset\n",
    "qnn_model = VQNNModel(num_of_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2020-01-26\", \"2023-04-10\", adjustment='raw').df\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['volume']\n",
    "\n",
    "\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"No rows with complete data found in the dataframe\")\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-11').tz_localize(pytz.utc)\n",
    "    x_train = np.array([], dtype=np.float32)\n",
    "    y_train = np.array([], dtype=np.float32)\n",
    "    x_test = np.array([], dtype=np.float32)\n",
    "    y_test = np.array([], dtype=np.float32)\n",
    "    if forward_test_date > df_x.index.max().tz_convert(pytz.utc):\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "        \n",
    " \n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                            df_y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            random_state=42)\n",
    "    if forward_test_date <= df_x.index.max().tz_convert(pytz.utc):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled,\n",
    "                                                        df_y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42)\n",
    "        x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "        y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "        x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "        y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "    else:\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "\n",
    "\n",
    "    x_train = np.expand_dims(x_train, 1).astype(np.float32)\n",
    "    y_train = np.expand_dims(y_train, 1).astype(np.float32)\n",
    "    x_validation = np.expand_dims(x_test, 1).astype(np.float32)\n",
    "    y_validation = np.expand_dims(y_test, 1).astype(np.float32)\n",
    "\n",
    "    qnn_model = VQNNModel()\n",
    "    qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1=0.9,\n",
    "                                                         beta_2=0.999,\n",
    "                                                         epsilon=1e-07),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[\"mean_squared_error\"])\n",
    "    qnn_model.run_eagerly = True\n",
    "\n",
    "    log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    qnn_model.fit(\n",
    "    x_train, y_train, epochs=1, batch_size=1,\n",
    "                  callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "    num_of_features = fdf_x_scaled.shape[1]\n",
    "    qnn_predictions = []\n",
    "    for entry in fdf_x_scaled.iterrows():\n",
    "        fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "        qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "        qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "    signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "\n",
    "\n",
    "    sub_qr = QuantumRegister(10)\n",
    "    sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "    sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "    sub_circuit.barrier()\n",
    "\n",
    "    # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "    sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "    ])\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483bb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-03-31 12:00:00+00:00', '2023-03-31 13:00:00+00:00',\n",
      "               '2023-03-31 14:00:00+00:00', '2023-03-31 15:00:00+00:00',\n",
      "               '2023-03-31 16:00:00+00:00', '2023-03-31 17:00:00+00:00',\n",
      "               '2023-03-31 18:00:00+00:00', '2023-03-31 19:00:00+00:00',\n",
      "               '2023-03-31 20:00:00+00:00', '2023-03-31 21:00:00+00:00',\n",
      "               '2023-03-31 22:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='timestamp', freq=None)\n"
     ]
    }
   ],
   "source": [
    "#This cell is meant to debug cell above\n",
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from tensor_layer_expermental import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "\n",
    "APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, APCA_API_NEWS_URL = \"\", \"\", \"\", \"\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ticker = \"INTC\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "def set_vars(secrets_file=\"secrets-alpaca.env\"):\n",
    "    \"\"\"\n",
    "    This function initializes the credentials by setting APCA_API_KEY_ID, APCA_API_SECRET_KEY, and APCA_API_BASE_URL\n",
    "    global variables from a specified .env file in the same directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    secrets_file: Default set to secrets-alpaca.env, can be any secrets file containing the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "\n",
    "    with open(f\"{secrets_file}\", 'r') as file:\n",
    "        contents = file.read()\n",
    "        env_vars = contents.replace('export ', '').split(\"\\n\")\n",
    "        APCA_API_KEY_ID = env_vars[0].split(\"=\")[1]\n",
    "        APCA_API_SECRET_KEY = env_vars[1].split(\"=\")[1]\n",
    "        APCA_API_BASE_URL = env_vars[2].split(\"=\")[1]\n",
    "        APCA_API_NEWS_URL = env_vars[2].split(\"=\")[1]\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    global APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL\n",
    "    set_vars()\n",
    "    api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL)\n",
    "    #test1 = api.get_aggs(\"INTC\", 1, 1,'2023-03-21','2023-03-31')\n",
    "    df = api.get_bars(ticker, TimeFrame.Hour, \"2023-03-31\", \"2023-03-31\", adjustment='raw').df\n",
    "    df['RETURNS'] = df['close'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['close'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['close'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(df.index)\n",
    "    \n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f001c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test date is outside the range of the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_18936\\1446791531.py:9: DeprecationWarning: The package qiskit.providers.ibmq is being deprecated. Please see https://ibm.biz/provider_migration_guide to get instructions on how to migrate to qiskit-ibm-provider (https://github.com/Qiskit/qiskit-ibm-provider) and qiskit-ibm-runtime (https://github.com/Qiskit/qiskit-ibm-runtime).\n",
      "  import QuantumDense\n",
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_18936\\1446791531.py:90: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  fdf_y = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "#from https://developers.refinitiv.com/en/article-catalog/article/tensorflow-variational-quantum-neural-networks-in-finance\n",
    "#In this cell data is obtained from yfinance\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "    \n",
    "now = date.date.today()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "\n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "    df['VOLUME'] = df['VOLUME']\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2023-04-10').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "        # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        # Use the trained model to make predictions\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "primaryfunc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90acc026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test date is outside the range of the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spark\\AppData\\Local\\Temp\\ipykernel_18936\\1005364885.py:77: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  fdf_y = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import QuantumDense\n",
    "import datetime as date\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytz\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "\n",
    "class VQNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(VQNNModel, self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.input_layer = Input(shape=(num_of_features,))\n",
    "        self.hidden_layer_1 = Dense(128)(self.input_layer)\n",
    "        self.activ_1 = Activation('relu')(self.hidden_layer_1)\n",
    "        self.hidden_layer_2 = Dense(64)(self.activ_1)\n",
    "        self.activ_2 = Activation('tanh')(self.hidden_layer_2)\n",
    "        self.hidden_layer_3 = Dense(32)(self.activ_2)\n",
    "        self.activ_3 = Activation('elu')(self.hidden_layer_3)\n",
    "        self.output_layer = Dense(1)(self.activ_3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        x = self.hidden_layer_3(x)\n",
    "        x = self.activ_3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def primaryfunc():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    now = date.date.today()\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "    yesterday = (now - pd.Timedelta('1day')).strftime('%Y-%m-%d')\n",
    "    ninedaysago = (now - pd.Timedelta('9day')).strftime('%Y-%m-%d')\n",
    "\n",
    "    # read input data from CSV file\n",
    "    df = pd.read_csv('datasets/EHC_5yr.csv')\n",
    "\n",
    "    # convert date column to datetime and set as index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.set_index('DATE', inplace=True)\n",
    "    df['HL_DELTA'] = df['HIGH'] - df['LOW']\n",
    "    df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "    df['PRX_MA_ND'] = df['CLOSE'].rolling(window=5).mean()\n",
    "    df['VOLATILITY'] = df['CLOSE'].rolling(window=5).std()\n",
    "    df['TP1_RETURNS'] = df['RETURNS'].shift(-1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # localize the timestamps to UTC, if not already timezone-aware\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "\n",
    "    df_x = df[['RETURNS', 'VOLUME', 'PRX_MA_ND', 'VOLATILITY']]\n",
    "    df_y = df['TP1_RETURNS']\n",
    "\n",
    "    df_x_scaler = MinMaxScaler().fit(df_x)\n",
    "\n",
    "    forward_test_date = pd.to_datetime('2025-03-01').tz_localize(pytz.utc)\n",
    "\n",
    "    df_x_scaled = pd.DataFrame()\n",
    "    fdf_x_scaled = pd.DataFrame()\n",
    "    if forward_test_date > df_x.index.max():\n",
    "        print(\"Test date is outside the range of the dataframe\")\n",
    "        fdf_x = pd.DataFrame()\n",
    "        fdf_y = pd.Series()\n",
    "    else:\n",
    "        fdf_x = df_x.loc[forward_test_date:]\n",
    "        fdf_y = df_y.loc[forward_test_date:]\n",
    "        df_x = df_x.loc[:forward_test_date]\n",
    "        df_y = df_y.loc[:forward_test_date]\n",
    "\n",
    "        df_x_scaled = pd.DataFrame(df_x_scaler.transform(df_x))\n",
    "        fdf_x_scaled = pd.DataFrame(df_x_scaler.transform(fdf_x))\n",
    "\n",
    "    if not df_x_scaled.empty and len(df_x_scaled) == len(df_y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, df_y, test_size=0.25, random_state=42)\n",
    "\n",
    "        if x_train is None:\n",
    "            print(\"x_train is None, returning default value\")\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.zeros((1, num_of_features, 1)).astype(np.float32)\n",
    "            y_train = np.zeros((1, 1, 1)).astype(np.float32)\n",
    "        else:\n",
    "            num_of_features = df_x.shape[1] # <-- Define num_of_features here\n",
    "            x_train = np.expand_dims(x_train.values, 1).astype(np.float32)\n",
    "            y_train = np.expand_dims(y_train.values, 1).astype(np.float32)\n",
    "            x_validation = np.expand_dims(x_test.values, 1).astype(np.float32)\n",
    "            y_validation = np.expand_dims(y_test.values, 1).astype(np.float32)\n",
    "\n",
    "        qnn_model = VQNNModel(num_of_features)\n",
    "        qnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                             beta_1=0.9,\n",
    "                                                             beta_2=0.999,\n",
    "                                                             epsilon=1e-07),\n",
    "                          loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          metrics=[\"mean_squared_error\"])\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        qnn_model.fit(\n",
    "            x_train, y_train, epochs=1, batch_size=1\n",
    "        )\n",
    "\n",
    "                # Define the number of features for the forward test data\n",
    "        num_of_features = fdf_x_scaled.shape[1]\n",
    "\n",
    "        qnn_predictions = []\n",
    "        for entry in fdf_x_scaled.iterrows():\n",
    "            fdf_x_predict_tensor = tf.reshape(tf.convert_to_tensor(entry[1]), [1, num_of_features])\n",
    "            qnn_forecast = qnn_model.predict(fdf_x_predict_tensor)\n",
    "            qnn_predictions.append(qnn_forecast[-1, -1, -1])\n",
    "\n",
    "        signal = [0 if x <= 0 else 1 for x in qnn_predictions]\n",
    "        sub_qr = QuantumRegister(10)\n",
    "        sub_circuit = QuantumCircuit(sub_qr, name='sub_circ')\n",
    "        sub_circuit.crz(1, sub_qr[0], sub_qr[5])\n",
    "        sub_circuit.barrier()\n",
    "\n",
    "        # Convert to a gate instruction and connect it with the QuantumDense registers\n",
    "        sub_instructions = sub_circuit.to_instruction()\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            QuantumDense(qubits=10, instructions=sub_instructions)\n",
    "        ])\n",
    "\n",
    "        log_dir = \"logs\\\\model\\\\eikon\\\\\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "primaryfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start over and build a VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
